{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad0642-c125-4869-b593-9341bdb9441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --target=temp_pip git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f1773-1b48-41e1-8739-2742c860ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --target=temp_pip commentjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578365c-3b9d-43f3-95e0-9374337d9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from math import log10, sqrt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def PSNR(original, compressed):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def SSIM(original, compressed):\n",
    "    return ssim(original, compressed, channel_axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f20dd-a7cd-482a-b1b1-2766a3af15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/global/u1/j/jswomley/tiny-cuda-nn/temp_pip\")\n",
    "\n",
    "try:\n",
    "\timport tinycudann as tcnn\n",
    "except ImportError:\n",
    "\tprint(\"This sample requires the tiny-cuda-nn extension for PyTorch.\")\n",
    "\tprint(\"You can install it by running:\")\n",
    "\tprint(\"============================================================\")\n",
    "\tprint(\"tiny-cuda-nn$ cd bindings/torch\")\n",
    "\tprint(\"tiny-cuda-nn/bindings/torch$ python setup.py install\")\n",
    "\tprint(\"============================================================\")\n",
    "\tsys.exit()\n",
    "\n",
    "SCRIPTS_DIR = \"/global/u1/j/jswomley/tiny-cuda-nn/scripts\" #os.path.join(os.path.dirname(os.path.dirname(__file__)), \"scripts\")\n",
    "sys.path.insert(0, SCRIPTS_DIR)\n",
    "print(sys.path)    \n",
    "    \n",
    "import argparse\n",
    "import commentjson as json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974fc6a-5d0d-405f-ac45-16104898b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import read_image, write_image, ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809b944-a83c-4839-a48d-9e464339d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class original_Image(torch.nn.Module):\n",
    "\tdef __init__(self, filename, device):\n",
    "\t\tsuper(original_Image, self).__init__()\n",
    "\t\tself.data = read_image(filename)\n",
    "\t\tself.shape = self.data.shape\n",
    "\t\tself.data = torch.from_numpy(self.data).float().to(device)\n",
    "\n",
    "\tdef forward(self, xs):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Bilinearly filtered lookup from the image. Not super fast,\n",
    "\t\t\t# but less than ~20% of the overall runtime of this example.\n",
    "\t\t\tshape = self.shape\n",
    "\n",
    "\t\t\txs = xs * torch.tensor([shape[1], shape[0]], device=xs.device).float()\n",
    "\t\t\tindices = xs.long()\n",
    "\t\t\tlerp_weights = xs - indices.float()\n",
    "\n",
    "\t\t\tx0 = indices[:, 0].clamp(min=0, max=shape[1]-1)\n",
    "\t\t\ty0 = indices[:, 1].clamp(min=0, max=shape[0]-1)\n",
    "\t\t\tx1 = (x0 + 1).clamp(max=shape[1]-1)\n",
    "\t\t\ty1 = (y0 + 1).clamp(max=shape[0]-1)\n",
    "\n",
    "\t\t\treturn (\n",
    "\t\t\t\tself.data[y0, x0] * (1.0 - lerp_weights[:,0:1]) * (1.0 - lerp_weights[:,1:2]) +\n",
    "\t\t\t\tself.data[y0, x1] * lerp_weights[:,0:1] * (1.0 - lerp_weights[:,1:2]) +\n",
    "\t\t\t\tself.data[y1, x0] * (1.0 - lerp_weights[:,0:1]) * lerp_weights[:,1:2] +\n",
    "\t\t\t\tself.data[y1, x1] * lerp_weights[:,0:1] * lerp_weights[:,1:2]\n",
    "\t\t\t)\n",
    "\n",
    "\n",
    "class Image(torch.nn.Module):\n",
    "\tdef __init__(self, filename, device, x_coord, y_coord, width, height):\n",
    "\t\tsuper(Image, self).__init__()\n",
    "\t\tself.data = read_image(filename)\n",
    "\t\tself.shape = self.data.shape\n",
    "\t\tself.data = torch.from_numpy(self.data).float().to(device)\n",
    "\t\tself.x_coord = x_coord\n",
    "\t\tself.y_coord = y_coord\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\n",
    "\tdef forward(self, xs):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Bilinearly filtered lookup from the image. Not super fast,\n",
    "\t\t\t# but less than ~20% of the overall runtime of this example.\n",
    "\t\t\t# shape = (self.x_shape, self.y_shape, 3)\n",
    "\t\t\tshape = self.shape\n",
    "            \n",
    "\t\t\treverse_output = xs\n",
    "\t\t\txs = xs * torch.tensor([shape[1], shape[0]], device=xs.device).float()\n",
    "\t\t\tindices = xs.long()\n",
    "\t\t\tlerp_weights = xs - indices.float()\n",
    "\n",
    "\t\t\tx0 = indices[:, 0].clamp(min=0, max=shape[1]-1)\n",
    "\t\t\ty0 = indices[:, 1].clamp(min=0, max=shape[0]-1)\n",
    "\t\t\tx1 = (x0 + 1).clamp(max=shape[1]-1)\n",
    "\t\t\ty1 = (y0 + 1).clamp(max=shape[0]-1)\n",
    "            \n",
    "\t\t\tout = (\n",
    "\t\t\t\tself.data[y0, x0] * (1.0 - lerp_weights[:,0:1]) * (1.0 - lerp_weights[:,1:2]) +\n",
    "\t\t\t\tself.data[y0, x1] * lerp_weights[:,0:1] * (1.0 - lerp_weights[:,1:2]) +\n",
    "\t\t\t\tself.data[y1, x0] * (1.0 - lerp_weights[:,0:1]) * lerp_weights[:,1:2] +\n",
    "\t\t\t\tself.data[y1, x1] * lerp_weights[:,0:1] * lerp_weights[:,1:2]\n",
    "\t\t\t)\n",
    "            \n",
    "            # For width/height only\n",
    "\t\t\t# x_vect = torch.where(x1 <= self.width, True, False)\n",
    "\t\t\t# y_vect = torch.where(y1 <= self.height, True, False)\n",
    "\t\t\t# vect = torch.logical_and(x_vect, y_vect)\n",
    "            \n",
    "            \n",
    "\t\t\tx0_vect = torch.where(x0 >= self.x_coord, True, False)\n",
    "\t\t\ty0_vect = torch.where(y0 >= self.y_coord, True, False)\n",
    "\t\t\tx1_vect = torch.where(x1 <= self.width + self.x_coord, True, False)\n",
    "\t\t\ty1_vect = torch.where(y1 <= self.height + self.y_coord, True, False)\n",
    "            \n",
    "\t\t\tvect = torch.logical_and(torch.logical_and(x0_vect, y0_vect), torch.logical_and(x1_vect, y1_vect))\n",
    "            \n",
    "\t\t\tout = out[vect, :]\n",
    "\t\t\treverse_output = reverse_output[vect, :]\n",
    "\t\t\t\n",
    "\t\t\treturn (out, reverse_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88e3b2-42c2-41ac-a862-36c9ab3e4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcnn.free_temporary_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e500d29-e464-40af-a32c-1617c50f1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Real ROI script\n",
    "\n",
    "# x_coord = 0; y_coord = 0; width=1000; height=1000\n",
    "\n",
    "# class Args:\n",
    "#     pass\n",
    "# args = Args()\n",
    "# args.image=\"/global/u1/j/jswomley/tiny-cuda-nn/data/images/hubble.jpg\"\n",
    "# args.image_roi=\"/global/u1/j/jswomley/tiny-cuda-nn/hubble_roi.jpg\"\n",
    "# args.config=\"/global/u1/j/jswomley/tiny-cuda-nn/config_hash.json\"\n",
    "# args.n_steps=10000\n",
    "# args.result_filename=\"/global/u1/j/jswomley/tiny-cuda-nn/result.png\"\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "# with open(args.config) as config_file:\n",
    "#     config = json.load(config_file)\n",
    "\n",
    "# image = Image(args.image, device, x_coord, y_coord, width, height)\n",
    "# n_channels = image.data.shape[2]\n",
    "\n",
    "# model = tcnn.NetworkWithInputEncoding(n_input_dims=2, n_output_dims=n_channels, encoding_config=config[\"encoding\"], network_config=config[\"network\"]).to(device)\n",
    "# # print(model)\n",
    "\n",
    "# #===================================================================================================\n",
    "# # The following is equivalent to the above, but slower. Only use \"naked\" tcnn.Encoding and\n",
    "# # tcnn.Network when you don't want to combine them. Otherwise, use tcnn.NetworkWithInputEncoding.\n",
    "# #===================================================================================================\n",
    "# # encoding = tcnn.Encoding(n_input_dims=2, encoding_config=config[\"encoding\"])\n",
    "# # network = tcnn.Network(n_input_dims=encoding.n_output_dims, n_output_dims=n_channels, network_config=config[\"network\"])\n",
    "# # model = torch.nn.Sequential(encoding, network)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Variables for saving/displaying image results\n",
    "# resolution = image.data.shape[0:2]\n",
    "\n",
    "# cut_resolution = torch.Size([height, width])\n",
    "# img_shape = cut_resolution + torch.Size([image.data.shape[2]])\n",
    "# real_img_shape = resolution + torch.Size([image.data.shape[2]])\n",
    "\n",
    "# n_pixels = resolution[0] * resolution[1]\n",
    "\n",
    "# half_dx =  0.5 / resolution[0]\n",
    "# half_dy =  0.5 / resolution[1]\n",
    "# xs = torch.linspace(half_dx, 1-half_dx, resolution[0], device=device)\n",
    "# ys = torch.linspace(half_dy, 1-half_dy, resolution[1], device=device)\n",
    "# xv, yv = torch.meshgrid([xs, ys])\n",
    "\n",
    "# xy = torch.stack((yv.flatten(), xv.flatten())).t()\n",
    "\n",
    "# path = f\"reference.jpg\"\n",
    "# print(f\"Writing '{path}'... \", end=\"\")\n",
    "# write_image(path, image(xy)[0].reshape(img_shape).detach().cpu().numpy())\n",
    "# print(\"done.\")\n",
    "\n",
    "# prev_time = time.perf_counter()\n",
    "\n",
    "# batch_size = 2**18\n",
    "# interval = 10\n",
    "\n",
    "# print(f\"Beginning optimization with {args.n_steps} training steps.\")\n",
    "\n",
    "# try:\n",
    "#     batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "#     traced_image = torch.jit.trace(image, batch)\n",
    "# except:\n",
    "#     # If tracing causes an error, fall back to regular execution\n",
    "#     print(f\"WARNING: PyTorch JIT trace failed. Performance will be slightly worse than regular.\")\n",
    "#     traced_image = image\n",
    "\n",
    "\n",
    "# for i in range(args.n_steps):\n",
    "#     batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "#     targets = traced_image(batch)[0]\n",
    "#     output = model(traced_image(batch)[1])\n",
    "\n",
    "#     relative_l2_error = (output - targets.to(output.dtype))**2 / (output.detach()**2 + 0.01)\n",
    "#     loss = relative_l2_error.mean()\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if i % interval == 0:\n",
    "#         loss_val = loss.item()\n",
    "#         torch.cuda.synchronize()\n",
    "#         elapsed_time = time.perf_counter() - prev_time\n",
    "#         print(f\"Step#{i}: loss={loss_val} time={int(elapsed_time*1000000)}[µs]\")\n",
    "\n",
    "#         path = f\"{i}.jpg\"\n",
    "#         print(f\"Writing '{path}'... \", end=\"\")\n",
    "#         with torch.no_grad():\n",
    "#             write_image(path, model(image(xy)[1]).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())\n",
    "#         print(\"done.\")\n",
    "\n",
    "#         # Ignore the time spent saving the image\n",
    "#         prev_time = time.perf_counter()\n",
    "\n",
    "#         if i > 0 and interval < 1000:\n",
    "#             interval *= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a981677-228d-463d-b721-c800fa2ef54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Original\n",
    "\n",
    "# class Args:\n",
    "#     pass\n",
    "# args = Args()\n",
    "# args.image=\"/global/u1/j/jswomley/tiny-cuda-nn/data/images/hubble.jpg\"\n",
    "# args.image_roi=\"/global/u1/j/jswomley/tiny-cuda-nn/hubble_roi.jpg\"\n",
    "# args.config=\"/global/u1/j/jswomley/tiny-cuda-nn/config_hash.json\"\n",
    "# args.n_steps=10000\n",
    "# args.result_filename=\"/global/u1/j/jswomley/tiny-cuda-nn/result.png\"\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "# with open(args.config) as config_file:\n",
    "#     config = json.load(config_file)\n",
    "\n",
    "# image = Image(args.image, device)\n",
    "# n_channels = image.data.shape[2]\n",
    "\n",
    "# model = tcnn.NetworkWithInputEncoding(n_input_dims=2, n_output_dims=n_channels, encoding_config=config[\"encoding\"], network_config=config[\"network\"]).to(device)\n",
    "# # print(model)\n",
    "\n",
    "# #===================================================================================================\n",
    "# # The following is equivalent to the above, but slower. Only use \"naked\" tcnn.Encoding and\n",
    "# # tcnn.Network when you don't want to combine them. Otherwise, use tcnn.NetworkWithInputEncoding.\n",
    "# #===================================================================================================\n",
    "# # encoding = tcnn.Encoding(n_input_dims=2, encoding_config=config[\"encoding\"])\n",
    "# # network = tcnn.Network(n_input_dims=encoding.n_output_dims, n_output_dims=n_channels, network_config=config[\"network\"])\n",
    "# # model = torch.nn.Sequential(encoding, network)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Variables for saving/displaying image results\n",
    "# resolution = image.data.shape[0:2]\n",
    "# img_shape = resolution + torch.Size([image.data.shape[2]])\n",
    "# n_pixels = resolution[0] * resolution[1]\n",
    "\n",
    "# half_dx =  0.5 / resolution[0]\n",
    "# half_dy =  0.5 / resolution[1]\n",
    "# xs = torch.linspace(half_dx, 1-half_dx, resolution[0], device=device)\n",
    "# ys = torch.linspace(half_dy, 1-half_dy, resolution[1], device=device)\n",
    "# xv, yv = torch.meshgrid([xs, ys])\n",
    "\n",
    "# xy = torch.stack((yv.flatten(), xv.flatten())).t()\n",
    "\n",
    "# path = f\"reference.jpg\"\n",
    "# print(f\"Writing '{path}'... \", end=\"\")\n",
    "# write_image(path, image(xy).reshape(img_shape).detach().cpu().numpy())\n",
    "# print(\"done.\")\n",
    "\n",
    "# prev_time = time.perf_counter()\n",
    "\n",
    "# batch_size = 2**18\n",
    "# interval = 10\n",
    "\n",
    "# print(f\"Beginning optimization with {args.n_steps} training steps.\")\n",
    "\n",
    "# try:\n",
    "#     batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "#     traced_image = torch.jit.trace(image, batch)\n",
    "# except:\n",
    "#     # If tracing causes an error, fall back to regular execution\n",
    "#     print(f\"WARNING: PyTorch JIT trace failed. Performance will be slightly worse than regular.\")\n",
    "#     traced_image = image\n",
    "\n",
    "\n",
    "# for i in range(args.n_steps):\n",
    "#     batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "#     targets = traced_image(batch)\n",
    "#     output = model(batch)\n",
    "\n",
    "#     relative_l2_error = (output - targets.to(output.dtype))**2 / (output.detach()**2 + 0.01)\n",
    "#     loss = relative_l2_error.mean()\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if i % interval == 0:\n",
    "#         loss_val = loss.item()\n",
    "#         torch.cuda.synchronize()\n",
    "#         elapsed_time = time.perf_counter() - prev_time\n",
    "#         print(f\"Step#{i}: loss={loss_val} time={int(elapsed_time*1000000)}[µs]\")\n",
    "\n",
    "#         path = f\"{i}.jpg\"\n",
    "#         print(f\"Writing '{path}'... \", end=\"\")\n",
    "#         with torch.no_grad():\n",
    "#             write_image(path, model(xy).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())\n",
    "#         print(\"done.\")\n",
    "\n",
    "#         # Ignore the time spent saving the image\n",
    "#         prev_time = time.perf_counter()\n",
    "\n",
    "#         if i > 0 and interval < 1000:\n",
    "#             interval *= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42903a-ad1e-4e5b-97db-f66c3eecff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Cropped\n",
    "\n",
    "class Args:\n",
    "    pass\n",
    "args = Args()\n",
    "args.image=\"/global/u1/j/jswomley/tiny-cuda-nn/data/images/hubble.jpg\"\n",
    "args.image_roi=\"/global/u1/j/jswomley/tiny-cuda-nn/hubble_roi.jpg\"\n",
    "args.config=\"/global/u1/j/jswomley/tiny-cuda-nn/config_hash.json\"\n",
    "args.n_steps=10000\n",
    "args.result_filename=\"/global/u1/j/jswomley/tiny-cuda-nn/result.png\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "with open(args.config) as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "image = original_Image(args.image, device)\n",
    "\n",
    "def naive_crop(x_coord, y_coord, width, height):\n",
    "    \n",
    "    image_roi_np = image.data[y_coord:y_coord+height, x_coord:x_coord+width, :].detach().cpu().numpy()\n",
    "    write_image(args.image_roi, image_roi_np)\n",
    "    image_roi = original_Image(args.image_roi, device)\n",
    "    \n",
    "    n_channels = image_roi.data.shape[2]\n",
    "\n",
    "    model = tcnn.NetworkWithInputEncoding(n_input_dims=2, n_output_dims=n_channels, encoding_config=config[\"encoding\"], network_config=config[\"network\"]).to(device)\n",
    "    print(model)\n",
    "\n",
    "    #===================================================================================================\n",
    "    # The following is equivalent to the above, but slower. Only use \"naked\" tcnn.Encoding and\n",
    "    # tcnn.Network when you don't want to combine them. Otherwise, use tcnn.NetworkWithInputEncoding.\n",
    "    #===================================================================================================\n",
    "    # encoding = tcnn.Encoding(n_input_dims=2, encoding_config=config[\"encoding\"])\n",
    "    # network = tcnn.Network(n_input_dims=encoding.n_output_dims, n_output_dims=n_channels, network_config=config[\"network\"])\n",
    "    # model = torch.nn.Sequential(encoding, network)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Variables for saving/displaying image results\n",
    "    resolution = image_roi.data.shape[0:2]\n",
    "    img_shape = resolution + torch.Size([image_roi.data.shape[2]])\n",
    "    n_pixels = resolution[0] * resolution[1]\n",
    "\n",
    "    half_dx =  0.5 / resolution[0]\n",
    "    half_dy =  0.5 / resolution[1]\n",
    "    xs = torch.linspace(half_dx, 1-half_dx, resolution[0], device=device)\n",
    "    ys = torch.linspace(half_dy, 1-half_dy, resolution[1], device=device)\n",
    "    xv, yv = torch.meshgrid([xs, ys])\n",
    "\n",
    "    xy = torch.stack((yv.flatten(), xv.flatten())).t()\n",
    "\n",
    "    path = f\"reference.jpg\"\n",
    "    print(f\"Writing '{path}'... \", end=\"\")\n",
    "    write_image(path, image_roi(xy).reshape(img_shape).detach().cpu().numpy())\n",
    "    print(\"done.\")\n",
    "\n",
    "    prev_time = time.perf_counter()\n",
    "\n",
    "    batch_size = 2**18\n",
    "    interval = 10\n",
    "\n",
    "    print(f\"Beginning optimization with {args.n_steps} training steps.\")\n",
    "\n",
    "    try:\n",
    "        batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "        traced_image = torch.jit.trace(image_roi, batch)\n",
    "    except:\n",
    "        # If tracing causes an error, fall back to regular execution\n",
    "        print(f\"WARNING: PyTorch JIT trace failed. Performance will be slightly worse than regular.\")\n",
    "        traced_image = image_roi\n",
    "        \n",
    "    steps = []; recon_psnr = []; recon_ssim = []; recon_loss = []; total_time = 0\n",
    "    \n",
    "    for i in range(args.n_steps):\n",
    "        batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "        targets = traced_image(batch)\n",
    "        output = model(batch)\n",
    "\n",
    "        relative_l2_error = (output - targets.to(output.dtype))**2 / (output.detach()**2 + 0.01)\n",
    "        loss = relative_l2_error.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % interval == 0:\n",
    "            loss_val = loss.item()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time = time.perf_counter() - prev_time\n",
    "            total_time += elapsed_time\n",
    "            print(f\"Step#{i}: loss={loss_val} time={int(elapsed_time*1000000)}[µs]\")\n",
    "            recon_psnr.append(PSNR(image_roi_np, model(xy).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())/100)\n",
    "            recon_ssim.append(SSIM(image_roi_np, model(xy).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy()))\n",
    "            recon_loss.append(loss_val)\n",
    "            steps.append(i)\n",
    "            \n",
    "            # path = f\"{i}.jpg\"\n",
    "            # print(f\"Writing '{path}'... \", end=\"\")\n",
    "            # with torch.no_grad():\n",
    "            #     write_image(path, model(xy).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())\n",
    "            # print(\"done.\")\n",
    "\n",
    "            # Ignore the time spent saving the image\n",
    "            prev_time = time.perf_counter()\n",
    "\n",
    "            if i > 0 and interval < 1000:\n",
    "                interval *= 10\n",
    "    \n",
    "    total_time += time.perf_counter() - prev_time\n",
    "    final_loss = loss.item()\n",
    "    if args.result_filename:\n",
    "        print(f\"Writing '{args.result_filename}'... \", end=\"\")\n",
    "        with torch.no_grad():\n",
    "            write_image(args.result_filename, model(xy).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())\n",
    "        print(\"done.\")\n",
    "    \n",
    "    tcnn.free_temporary_memory()\n",
    "    return [steps, recon_psnr, recon_ssim, recon_loss, final_loss, total_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41139cef-0225-4730-9371-97549ab0bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real ROI function\n",
    "\n",
    "def recon_roi(x_coord, y_coord, width, height):\n",
    "    #coordinates count from top left\n",
    "\n",
    "    class Args:\n",
    "        pass\n",
    "    args = Args()\n",
    "    args.image=\"/global/u1/j/jswomley/tiny-cuda-nn/data/images/hubble.jpg\"\n",
    "    args.image_roi=\"/global/u1/j/jswomley/tiny-cuda-nn/hubble_roi.jpg\"\n",
    "    args.config=\"/global/u1/j/jswomley/tiny-cuda-nn/config_hash.json\"\n",
    "    args.n_steps=10000\n",
    "    args.result_filename=\"/global/u1/j/jswomley/tiny-cuda-nn/result.png\"\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    with open(args.config) as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    image = Image(args.image, device, x_coord, y_coord, width, height)\n",
    "    n_channels = image.data.shape[2]\n",
    "    image_roi_np = image.data[y_coord:y_coord+height, x_coord:x_coord+width, :].detach().cpu().numpy()\n",
    "\n",
    "    model = tcnn.NetworkWithInputEncoding(n_input_dims=2, n_output_dims=n_channels, encoding_config=config[\"encoding\"], network_config=config[\"network\"]).to(device)\n",
    "    # print(model)\n",
    "\n",
    "    #===================================================================================================\n",
    "    # The following is equivalent to the above, but slower. Only use \"naked\" tcnn.Encoding and\n",
    "    # tcnn.Network when you don't want to combine them. Otherwise, use tcnn.NetworkWithInputEncoding.\n",
    "    #===================================================================================================\n",
    "    # encoding = tcnn.Encoding(n_input_dims=2, encoding_config=config[\"encoding\"])\n",
    "    # network = tcnn.Network(n_input_dims=encoding.n_output_dims, n_output_dims=n_channels, network_config=config[\"network\"])\n",
    "    # model = torch.nn.Sequential(encoding, network)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Variables for saving/displaying image results\n",
    "    resolution = image.data.shape[0:2]\n",
    "\n",
    "    cut_resolution = torch.Size([height, width])\n",
    "    img_shape = cut_resolution + torch.Size([image.data.shape[2]])\n",
    "    real_img_shape = resolution + torch.Size([image.data.shape[2]])\n",
    "\n",
    "    n_pixels = resolution[0] * resolution[1]\n",
    "\n",
    "    half_dx =  0.5 / resolution[0]\n",
    "    half_dy =  0.5 / resolution[1]\n",
    "    xs = torch.linspace(half_dx, 1-half_dx, resolution[0], device=device)\n",
    "    ys = torch.linspace(half_dy, 1-half_dy, resolution[1], device=device)\n",
    "    xv, yv = torch.meshgrid([xs, ys])\n",
    "\n",
    "    xy = torch.stack((yv.flatten(), xv.flatten())).t()\n",
    "\n",
    "    path = f\"reference.jpg\"\n",
    "    print(f\"Writing '{path}'... \", end=\"\")\n",
    "    write_image(path, image(xy)[0].reshape(img_shape).detach().cpu().numpy())\n",
    "    print(\"done.\")\n",
    "\n",
    "    prev_time = time.perf_counter()\n",
    "\n",
    "    batch_size = 2**18\n",
    "    interval = 10\n",
    "\n",
    "    print(f\"Beginning optimization with {args.n_steps} training steps.\")\n",
    "\n",
    "    try:\n",
    "        batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "        traced_image = torch.jit.trace(image, batch)\n",
    "    except:\n",
    "        # If tracing causes an error, fall back to regular execution\n",
    "        print(f\"WARNING: PyTorch JIT trace failed. Performance will be slightly worse than regular.\")\n",
    "        traced_image = image\n",
    "\n",
    "        \n",
    "    steps = []; recon_psnr = []; recon_ssim = []; recon_loss = []; total_time = 0\n",
    "    \n",
    "    for i in range(args.n_steps):\n",
    "        batch = torch.rand([batch_size, 2], device=device, dtype=torch.float32)\n",
    "        targets = traced_image(batch)[0]\n",
    "        output = model(traced_image(batch)[1])\n",
    "\n",
    "        relative_l2_error = (output - targets.to(output.dtype))**2 / (output.detach()**2 + 0.01)\n",
    "        loss = relative_l2_error.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % interval == 0:\n",
    "            loss_val = loss.item()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time = time.perf_counter() - prev_time\n",
    "            total_time += elapsed_time\n",
    "            print(f\"Step#{i}: loss={loss_val} time={int(elapsed_time*1000000)}[µs]\")\n",
    "            recon_psnr.append(PSNR(image_roi_np, model(image(xy)[1]).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())/100)\n",
    "            recon_ssim.append(SSIM(image_roi_np, model(image(xy)[1]).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy()))\n",
    "            recon_loss.append(loss_val)\n",
    "            steps.append(i)\n",
    "            \n",
    "            # path = f\"{i}.jpg\"\n",
    "            # print(f\"Writing '{path}'... \", end=\"\")\n",
    "            # with torch.no_grad():\n",
    "            #     write_image(path, model(xy).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())\n",
    "            # print(\"done.\")\n",
    "\n",
    "            # Ignore the time spent saving the image\n",
    "            prev_time = time.perf_counter()\n",
    "\n",
    "            if i > 0 and interval < 1000:\n",
    "                interval *= 10\n",
    "    \n",
    "    total_time += time.perf_counter() - prev_time\n",
    "    final_loss = loss.item()\n",
    "    if args.result_filename:\n",
    "        print(f\"Writing '{args.result_filename}'... \", end=\"\")\n",
    "        with torch.no_grad():\n",
    "            write_image(args.result_filename, model(image(xy)[1]).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy())\n",
    "        print(\"done.\")\n",
    "    \n",
    "    tcnn.free_temporary_memory()\n",
    "    return [steps, recon_psnr, recon_ssim, recon_loss, final_loss, total_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bf32d-08fb-4982-9693-787ea13d3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokyo_cropped\n",
    "# quarter_metrics = recon_roi(0, 0, 6250, 7000)\n",
    "# half_metrics = recon_roi(0, 0, 12500, 7000)\n",
    "# three_quarter_metrics = recon_roi(0, 0, 18750, 7000)\n",
    "# full_metrics = recon_roi(0, 0, 25000, 7000)\n",
    "\n",
    "### Hubble\n",
    "# quarter_metrics = recon_roi(0,0,1695,7071)\n",
    "# half_metrics = recon_roi(0,0,3390,7071)\n",
    "# three_quarter_metrics = recon_roi(0,0,5085,7071)\n",
    "# full_metrics = recon_roi(0,0,6780,7071)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592927f-c735-40d0-82a1-972deb567b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcnn.free_temporary_memory()\n",
    "real_metrics = recon_roi(2000, 2000, 20, 20)\n",
    "tcnn.free_temporary_memory()\n",
    "naive_metrics = naive_crop(2000, 2000, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973bef0-a38e-409d-85bf-4725f0bcef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_metrics(metrics):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(metrics[0], metrics[1], label=\"PSNR\")\n",
    "    plt.plot(metrics[0], metrics[2], label=\"SSIM\")\n",
    "    plt.xlabel(\"recon step\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(metrics[0], metrics[3], label=\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"final loss: {}\\nfinal PSNR: {}\\nfinal SSIM: {}\\ntotal time: {} seconds\".format(metrics[4], metrics[1][-1], metrics[2][-1], metrics[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec04fd-c4cd-4492-9192-aa575d7097fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_metrics(real_metrics)\n",
    "view_metrics(naive_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17961cdb-23a8-45f2-8156-ea97811fce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = torch.Tensor.cpu(encoding.params.data)\n",
    "# print(enc, enc.shape, enc.dtype)\n",
    "# weights = torch.Tensor.cpu(network.params.data)\n",
    "# print(f\"{enc.shape[0]*4/1e6} MB + {weights.shape[0]*4/1e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03941a0-9557-4175-bbec-8eb33cdb3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = tcnn.Encoding(n_input_dims=2, encoding_config=config[\"encoding\"],seed=0)\n",
    "# network = tcnn.Network(n_input_dims=encoding.n_output_dims, n_output_dims=n_channels, network_config=config[\"network\"],seed=0)\n",
    "# model1 = torch.nn.Sequential(encoding, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755b5eb-3c70-4187-a342-e57a475a91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{image.shape[0]*image.shape[1]*image.shape[2]/1e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f735c2c-112f-40dc-b8dd-06f557f058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model(xy).reshape(img_shape).clamp(0.0, 1.0).detach().cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3f16a-21ce-4f54-b4a4-4f4766155092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = image.data.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d772a5d-73ef-4a2c-a396-1f3294e9260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.max(a), np.max(b))\n",
    "# print(np.min(a), np.min(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec435fbd-b529-4895-9a66-e6d448ae2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c=a-b\n",
    "# print(np.max(c), np.min(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d9a75-fdd5-4062-8f4a-1fd457da744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(121)\n",
    "# plt.imshow(b); plt.title(\"original\"); \n",
    "# plt.subplot(122)\n",
    "# plt.imshow(a); plt.title(\"compressed\");\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1e069-2277-47d7-9742-84f28fb0fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(121)\n",
    "# plt.imshow(b[2500:3000,2600:2900]); plt.title(\"original\"); \n",
    "# plt.subplot(122)\n",
    "# plt.imshow(a[2500:3000,2600:2900]); plt.title(\"compressed\");\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cba2d-8192-4d03-9c85-eb20b6126144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_image(\"compressed.png\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b01aa-7b04-4f13-aa46-9af5e5c57cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_image(\"compressed_zoom.png\",a[2500:3000,2600:2900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be558900-bfa3-4cbb-a10d-b167bef0c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_image(\"original_zoom.png\",b[2500:3000,2600:2900])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.13.1",
   "language": "python",
   "name": "pytorch-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
